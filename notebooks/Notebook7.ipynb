{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap mean: 12.45\n",
      "95% confidence interval: [10. 15.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume these are the commit sizes for two commits where the file is present\n",
    "commit_sizes_present = np.array([10, 15])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap_samples = 1000\n",
    "\n",
    "# Bootstrap sampling\n",
    "bootstrap_means = np.random.choice(commit_sizes_present, (n_bootstrap_samples, len(commit_sizes_present)), replace=True).mean(axis=1)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "conf_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(f\"Bootstrap mean: {np.mean(bootstrap_means)}\")\n",
    "print(f\"95% confidence interval: {conf_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group: 63.765611775409695\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "# Define parameters\n",
    "effect_size = 0.5  # Cohen's d, a medium effect size\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "\n",
    "# Calculate sample size\n",
    "analysis = TTestIndPower()\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
    "\n",
    "print(f\"Required sample size per group: {sample_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate required sample size per group (t-test): 33.0245695315096\n",
      "Approximate required sample size per group (Mann-Whitney U test): 37.97825496123603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.stats.power as smp\n",
    "\n",
    "# Parameters\n",
    "effect_size = 0.7  # Example medium effect size\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "\n",
    "# Calculate the sample size for t-test (as an approximation)\n",
    "analysis = smp.TTestIndPower()\n",
    "sample_size_ttest = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
    "\n",
    "print(f\"Approximate required sample size per group (t-test): {sample_size_ttest}\")\n",
    "\n",
    "# Convert t-test sample size to Mann-Whitney U test sample size using a correction factor\n",
    "# According to some literature, the sample size for Mann-Whitney U can be about 1.15 times that of a t-test for similar power\n",
    "sample_size_mannwhitney = sample_size_ttest * 1.15\n",
    "\n",
    "print(f\"Approximate required sample size per group (Mann-Whitney U test): {sample_size_mannwhitney}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group: 28\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "import numpy as np\n",
    "\n",
    "# Function to perform simulation\n",
    "def power_simulation(effect_size, n1, n2, alpha, num_simulations=1000):\n",
    "    power_count = 0\n",
    "    for _ in range(num_simulations):\n",
    "        group1 = np.random.normal(0, 1, n1)\n",
    "        group2 = np.random.normal(effect_size, 1, n2)\n",
    "        stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "        if p_value < alpha:\n",
    "            power_count += 1\n",
    "    return power_count / num_simulations\n",
    "\n",
    "# Initial parameters\n",
    "effect_size = 0.8\n",
    "alpha = 0.05\n",
    "power_target = 0.8\n",
    "num_simulations = 5000\n",
    "\n",
    "# Iteratively find the sample size\n",
    "sample_size = 1\n",
    "achieved_power = 0\n",
    "while achieved_power < power_target:\n",
    "    achieved_power = power_simulation(effect_size, sample_size, sample_size, alpha, num_simulations)\n",
    "    sample_size += 1\n",
    "\n",
    "print(f\"Required sample size per group: {sample_size - 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Rankings (sorted by rank):\n",
      "1. file4 - Average Commit Size: 3.0, Confidence: nan\n",
      "2. file3 - Average Commit Size: 2.5, Confidence: 0.5000\n",
      "3. file2 - Average Commit Size: 2.5, Confidence: 0.5000\n",
      "4. file1 - Average Commit Size: 2.0, Confidence: nan\n",
      "5. file5 - Average Commit Size: 1.0, Confidence: nan\n",
      "\n",
      "Threshold (Confidence): 0.05\n",
      "Threshold (Commit Size): 2.71\n",
      "\n",
      "Significant Files (based on confidence threshold): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtl86/source/python/entitybert/.venv/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/Users/jtl86/source/python/entitybert/.venv/lib/python3.12/site-packages/scipy/stats/_stats_py.py:1087: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "/Users/jtl86/source/python/entitybert/.venv/lib/python3.12/site-packages/scipy/stats/_stats_py.py:1087: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Given sets\n",
    "R = {\"commit1\", \"commit2\", \"commit3\", \"commit4\"}\n",
    "F = {\"file1\", \"file2\", \"file3\", \"file4\", \"file5\"}\n",
    "C = {(\"commit1\", \"file1\"), (\"commit1\", \"file2\"), (\"commit2\", \"file1\"), \n",
    "     (\"commit2\", \"file3\"), (\"commit3\", \"file2\"), (\"commit3\", \"file3\"), \n",
    "     (\"commit3\", \"file4\"), (\"commit4\", \"file5\")}\n",
    "\n",
    "# Step 1: Calculate commit sizes\n",
    "commit_sizes = {r: 0 for r in R}\n",
    "for (r, f) in C:\n",
    "    commit_sizes[r] += 1\n",
    "\n",
    "# Step 2: Collect commit sizes for each file\n",
    "file_commit_sizes = {f: [] for f in F}\n",
    "for (r, f) in C:\n",
    "    file_commit_sizes[f].append(commit_sizes[r])\n",
    "\n",
    "# Step 3: Calculate average commit size for each file\n",
    "file_avg_commit_size = {f: np.mean(sizes) if sizes else 0 for f, sizes in file_commit_sizes.items()}\n",
    "\n",
    "# Step 4: Rank files based on average commit size\n",
    "file_ranks = sorted(file_avg_commit_size.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 5: Perform hypothesis testing to assign confidence scores\n",
    "all_commit_sizes = list(commit_sizes.values())\n",
    "file_confidences = {}\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "for f, sizes in file_commit_sizes.items():\n",
    "    if sizes:\n",
    "        t_stat, p_value = ttest_1samp(sizes, np.mean(all_commit_sizes))\n",
    "        file_confidences[f] = p_value\n",
    "    else:\n",
    "        file_confidences[f] = 1  # High p-value for files with no changes\n",
    "\n",
    "# Step 6: Determine threshold for significance\n",
    "threshold_confidence = alpha\n",
    "threshold_commit_size = np.mean(all_commit_sizes) + np.std(all_commit_sizes)\n",
    "\n",
    "# Filter files by confidence threshold\n",
    "significant_files = [f for f, p_value in file_confidences.items() if p_value < threshold_confidence]\n",
    "\n",
    "# Display the results\n",
    "print(\"File Rankings (sorted by rank):\")\n",
    "for rank, (file, avg_size) in enumerate(file_ranks, 1):\n",
    "    print(f\"{rank}. {file} - Average Commit Size: {avg_size}, Confidence: {file_confidences[file]:.4f}\")\n",
    "\n",
    "print(f\"\\nThreshold (Confidence): {threshold_confidence}\")\n",
    "print(f\"Threshold (Commit Size): {threshold_commit_size:.2f}\")\n",
    "print(\"\\nSignificant Files (based on confidence threshold):\", significant_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
